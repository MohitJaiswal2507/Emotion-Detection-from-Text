# ğŸ§  Hybrid BERT-RoBERTa Ensemble for Emotion Classification

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)  
[![Transformers](https://img.shields.io/badge/Transformers-4.x-yellow.svg)](https://huggingface.co/docs/transformers/index)  
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)  
[![License](https://img.shields.io/badge/License-MIT-green.svg)](./LICENSE)  

---

## ğŸš€ Overview

This project implements a **Hybrid BERT-RoBERTa Ensemble** designed for high-granularity **Text Emotion Classification (TEC)**. While traditional models are often limited to basic emotions, this architecture captures 13 nuanced human states including **sarcasm, confusion, and shame**.

By concatenating the 768-dimensional CLS feature embeddings from both `bert-base-uncased` and `roberta-base`, the model creates a rich **1536-dimensional feature vector** for superior classification performance.

---

## âœ¨ Features

- ğŸ­ **Nuanced Detection** â†’ Trained on a 13-label dataset including Sarcasm, Guilt, and Confusion.
- ğŸ¤– **Ensemble Architecture** â†’ Parallel fine-tuning of BERT and RoBERTa models.
- ğŸ“Š **Superior Performance** â†’ Achieves a validation accuracy of **69.93%**, outperforming single-model baselines.
- ğŸ§ª **Research-Backed** â†’ Based on methodology developed at **Bennett University**.

---

## ğŸ—ï¸ Architecture

The model processes text through dual pipelines, concatenating the final hidden states (CLS tokens) before passing them through a linear classifier.

1. **Preprocessing**: Dual tokenization using BERT and RoBERTa tokenizers.
2. **Feature Extraction**: Parallel processing through pre-trained Transformer layers.
3. **Fusion**: Concatenation of embeddings into a 1536-dimensional vector.
4. **Classification**: Final linear layer for 13-class emotion prediction.

---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the Repository
```bash
git clone [https://github.com/MohitJaiswal2507/Emotion-Detection-from-Text](https://github.com/MohitJaiswal2507/Emotion-Detection-from-Text)
cd Emotion-Detection-from-Text
```

### 2ï¸âƒ£ Create a Virtual Environment

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

```

### 3ï¸âƒ£ Install Dependencies 

```bash
pip install -r requirements.txt

```

### 4ï¸âƒ£ Training (train.ipynb)
Open and run the train.ipynb notebook. This script:

* Loads the Emotions Dataset.

* Performs dual tokenization for BERT and RoBERTa.

* Trains the hybrid model for N(any number of your choice) epochs on any GPU 

* Saves the final model weights to the models/ directory.

### 5ï¸âƒ£ Once Training is complete, Run :
   ```bash
python app.py

```


## ğŸ›  Tech Stack

* **Frameworks**: PyTorch, Hugging Face Transformers.


* **Models**: BERT (bert-base-uncased) & RoBERTa (roberta-base).


* **Hardware**: Training performed on **Nvidia RTX 4060 GPU**.


* **Data**: Emotions Dataset by boltuix (130k+ samples).


---

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ train.ipynb           # Model training & architecture logic
â”œâ”€â”€ app.py                # Inference script/Application
â”œâ”€â”€ requirements.txt      # Project dependencies
â”œâ”€â”€ data/                 # Dataset directory
â”œâ”€â”€ models/               # Saved model weights
â”œâ”€â”€ screenshots/          # Performance graphs & Confusion Matrix
â”œâ”€â”€ .gitignore            # Files to ignore (e.g., __pycache__)
â””â”€â”€ Emotion_Detection_from_text.pdf # Research Paper

```

---

## ğŸ‘¨â€ğŸ’» Author

**Mohit Jaiswal** ğŸ“§ [mohitjaiswal2507@gmail.com](mailto:mohitjaiswal2507@gmail.com)
---

## ğŸ“œ License

This project is licensed under the **MIT License**. See the [LICENSE](https://www.google.com/search?q=./LICENSE) file for details.

